{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hdWMBjFpKDg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKgdK53RqWhq"
   },
   "source": [
    "In this tutorial we will show you how to construct a model in pytorch, train a model and subsequently use it to make predictions.\n",
    "\n",
    "The tutorial will use the standard dataset used in digit classification, i.e. *MNIST*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWpFJxt0N2BR"
   },
   "source": [
    "# Pytorch cheat sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "owzvnY8nN5yi"
   },
   "source": [
    "https://pytorch.org/tutorials/beginner/ptcheat.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2DTbFhdpN-o"
   },
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zkyw6Yg3pch6"
   },
   "source": [
    "## Construction of models using predefined layers and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hxKc7HQrBe_"
   },
   "source": [
    "Pytorch is a machine learning package that constructs it's models as a subclass of the torch.nn.Module class. A model subclass requires the following components in order to be useable with the library\n",
    "\n",
    "\n",
    "1.   **__ init __**, contains all layers and their trainable parameters that will be utilised in the network\n",
    "2.   **forward**, a function that describes the computation that the network needs to follow in order to complete a forward propagation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjRDdmmBpSFm"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    #This defines the structure of the NN.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()  #Dropout\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Convolutional Layer/Pooling Layer/Activation\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        #Convolutional Layer/Dropout/Pooling Layer/Activation\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        #Fully Connected Layer/Activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        #Fully Connected Layer/Activation\n",
    "        x = self.fc2(x)\n",
    "        #Softmax gets probabilities. \n",
    "        return F.log_softmax(x, dim=1)\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8omXaHLpl7Y"
   },
   "source": [
    "## Writing custom layers and activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1I6O7EVudCX"
   },
   "source": [
    "Custom layers need to be written as a subclass of the nn.Module similary as to how a network is defined. Like the definition of a network it needs an init and forward function.\n",
    "\n",
    "The main difference however is that in the init function we define the learnable parameters and their initialisation method\n",
    "\n",
    "The code cell belows depicts the implementation of the prewritten linear mapping layer **nn.Linear** in the torch standard library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNZllsA3pSe6"
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        self.weight.data.uniform_(-0.1, 0.1)\n",
    "        if bias is not None:\n",
    "            self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yt0ZaaGvIBD"
   },
   "source": [
    "Writing custom functions to be used in the network requires that you write subclasses of the Function. This class needs to be defined with\n",
    "\n",
    "\n",
    "1.   **forward**, which describes the forward propagation of the input \n",
    "2.   **backward**, which describes how the gradients are to be computed with respect to all the inputs received for this function.\n",
    "\n",
    "The code cell belows depicts the implementation fo the linear function in the nn.Function module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEbOvo8hvwGy"
   },
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # ctx is a context object that can be used to \n",
    "    # stash information for backward computation,\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            # this will expand the dimension of the bias from (dim,) to (dim,1)\n",
    "            # .expand_as(output) to get it in the same dimensions as output\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aP50F1PkwiZC"
   },
   "source": [
    "For a more in depth discussion on the extension of torch check https://pytorch.org/docs/stable/notes/extending.html?highlight=linearfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2A6IYHgopS-A"
   },
   "source": [
    "# Model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tcBfVniVKPIP"
   },
   "source": [
    "\n",
    "From Kaggle: \n",
    "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
    "\n",
    "[Read more.](https://www.kaggle.com/c/digit-recognizer)\n",
    "\n",
    "\n",
    "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Row8i-Aepsd-"
   },
   "source": [
    "# Loading data and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7iT8CW9PyLO"
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGI5VochLSJw"
   },
   "source": [
    "Pytorch makes use of the DataLoader class which forms a pipeline that is utilized during the optimization of the network\n",
    "\n",
    "```\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "```\n",
    "Which gives a an iteratable. This class enables a parallelized datastream that allows for multiple CPUs to simultaneously feed data to the GPU.\n",
    "\n",
    "The dataset keyword is a subclass of the the data.Dataset class and can be written for custom datasets (see additional resources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "kHCs7pDLpWGt",
    "outputId": "fa3943e8-ee5f-4d47-a6a9-d6c23f18cbd5"
   },
   "outputs": [],
   "source": [
    "train_batch = 64\n",
    "test_batch = 64\n",
    "\n",
    "kwargs = {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
    "                                                          transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                          batch_size=train_batch, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, \n",
    "                                                         transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                                          batch_size=test_batch, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fr_A2KWyPOQK"
   },
   "source": [
    "**Additional resources**\n",
    "\n",
    "https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "\n",
    "https://towardsdatascience.com/building-efficient-custom-datasets-in-pytorch-2563b946fd9f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIXFq_HOpxSL"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNELnNwDUbDa"
   },
   "source": [
    "### Optimization and inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bEEphgBip6VK"
   },
   "outputs": [],
   "source": [
    "def train(epoch, log_interval, cuda=False):\n",
    "    #Set the internal state of the model to training mode\n",
    "    #Some layers behabe differently during training then during the evaluation mode\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda is True:\n",
    "            # changing the the variables data,target to another device (GPU)\n",
    "            # all variables (+model) need to be on the same device!\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #Variables in Pytorch are differenciable. \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # set the gradients to zero before starting to do backpropragation \n",
    "        # because PyTorch accumulates the gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #dloss/dx for every Variable \n",
    "        loss.backward()\n",
    "        #to do a one-step update on our parameter.\n",
    "        optimizer.step()\n",
    "        #Print out the loss periodically. \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cn-gqOjRIyl"
   },
   "outputs": [],
   "source": [
    "def test(cuda=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda is True:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpzemgdDUidK"
   },
   "source": [
    "### GPU allocation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2FhsKwsS6JY"
   },
   "source": [
    "After defining the training and test function we can start to train the model. However, if we wish to train on available then we need to copy our model to an available gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CradkTUmSv4H",
    "outputId": "5e42c948-cf74-4a18-e681-4e715a8df107"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LA_KL-z2T-O9"
   },
   "source": [
    "Should a CUDA compatible device be available we can create our model on said device either through the \n",
    "\n",
    "\n",
    "*   .cuda() command\n",
    "*   .to_device(*device_name*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "3tCiEw4GTZhq",
    "outputId": "46bdc7a7-6571-4688-d441-3e3089612545"
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exB1Vq9SUPwY"
   },
   "source": [
    "And we can check whether it has in fact been allocated to the GPU by checking the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oIOFVEGRTl0r",
    "outputId": "8939b670-917d-4412-faba-743b1d18c9d8"
   },
   "outputs": [],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDLnIqcjUnUy"
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SD2PIDipUs3p",
    "outputId": "83233aa5-fcbd-4b08-9a72-15e00ec35ead"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "for epoch in range(1,100):\n",
    "    train(epoch, 100,cuda=True)\n",
    "    test(cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JBrBfmYpWoi"
   },
   "source": [
    "# Post optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6ryes9Kp5mA"
   },
   "source": [
    "## Saving models and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2i6nrvGdIDO"
   },
   "source": [
    "When saving a model for inference, it is only necessary to save the trained model’s learned parameters. Saving the model’s state_dict with the torch.save() function will give you the most flexibility for restoring the model later, which is why it is the recommended method for saving models.\n",
    "\n",
    "A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
    "\n",
    "\n",
    "    torch.save: Saves a serialized object to disk. This function uses Python’s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "    torch.load: Uses pickle’s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into (see Saving & Loading Model Across Devices).\n",
    "    torch.nn.Module.load_state_dict: Loads a model’s parameter dictionary using a deserialized state_dict. For more information on state_dict, see What is a state_dict?.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WSGzm5K-pbU7"
   },
   "outputs": [],
   "source": [
    "#Save the models state dictionary to the specified PATH\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "#Reload the model by loading in the state dictionary and subsequently updating the state dict of your model class\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "vpzemgdDUidK"
   ],
   "name": "Pytorch tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
